- [收集容器内日志](#org97e6894)
      - [注意](#org7371055)
      - [完整的 fluentd.conf 配置](#orge12aaa9)
    - [扩展阅读](#orgd0dbd52)
      - [fluentd process nested JSON](#org4011f94)
        - [eg](#org7f12289)
      - [参考](#org4d36401)
        - [sonots/fluent-plugin-record-reformer](#org77adda8)
        - [日志收集工具 Fluentd 使用总结](#org8427bdb)
        - [k8s 应用内部日志收集](#org7b81aee)
        - [AliyunContainerService/fluentd-pilot:](#org80b72cd)

~~+~~ categories = ["log","kubernetes"] tags = ["fluentd","k8s", "kubernetes"] type = "post" title = "fluentd 容器内日志收集解决方案" subtitle = "fluentd 容器内日志收集解决方案" date = "2017-07-29" draft = false description= "收集容器中的日志写入文件的解决方案" ~~+~~

fluentd 容器内日志收集，采用使用 agent 将容器日志文件映射到特定目录的方式，然后通过 fluentd 抓取，并将日志存储到相应的存储系统中，如 es，hadoop，mysql，es，mongoDB 等。


<a id="org97e6894"></a>

# 收集容器内日志

统一将一个 node(节点) 上容器内的日志通过做软连接的方式放在/var/log/containers/applog/下， 在 applog 下文件的软连接的格式如下:

```sh
<volumeName>.<PodName>_<Namespace>_<containerName>-<containerId>.log.<logFileName>
appvolume.ubuntu-write-file_default_ubuntu-write-file-container-79e84190d88a0e639309a05f3c93bf14411e471cf7552f304fe3d3b6178ad87e.log.test_file-xx0xx.234.txt
```

对上面格式解析：

-   <volumeName>: appvolume
-   <PodName> : ubuntu-write-file
-   <NameSpace>: default
-   <containerName>: ubuntu-write-file-container
-   <containerId>: 79e84190d88a0e639309a05f3c93bf14411e471cf7552f304fe3d3b6178ad87e
-   <logFileName>: test<sub>file</sub>-xx0xx.234.txt


<a id="org7371055"></a>

## 注意

在 log 和文件名之间以"."分割


<a id="orge12aaa9"></a>

## 完整的 fluentd.conf 配置

```yaml
<source>
  type tail
  path /var/log/containers/*.log
  pos_file /var/log/es-containers.log.pos
  time_format %Y-%m-%dT%H:%M:%S
  tag kubernetes.*
  format json
  read_from_head true
</source>

<source>
  type tail
  format none
  path /var/log/salt/minion
  pos_file /var/log/gcp-salt.pos
  tag salt
</source>

<source>
  type tail
  format none
  path /var/log/startupscript.log
  pos_file /var/log/es-startupscript.log.pos
  tag startupscript
</source>

<source>
  type tail
  format none
  path /var/log/docker.log
  pos_file /var/log/es-docker.log.pos
  tag docker
</source>

<source>
  type tail
  format none
  path /var/log/etcd.log
  pos_file /var/log/es-etcd.log.pos
  tag etcd
</source>

<source>
  type tail
  format none
  path /var/log/kubelet.log
  pos_file /var/log/es-kubelet.log.pos
  tag kubelet
</source>

<source>
  type tail
  format none
  path /var/log/kube-apiserver.log
  pos_file /var/log/es-kube-apiserver.log.pos
  tag kube-apiserver
</source>

<source>
  type tail
  format none
  path /var/log/kube-controller-manager.log
  pos_file /var/log/es-kube-controller-manager.log.pos
  tag kube-controller-manager
</source>

<source>
  type tail
  format none
  path /var/log/kube-scheduler.log
  pos_file /var/log/es-kube-scheduler.log.pos
  tag kube-scheduler
</source>

<source>
  type tail
  format none
  path /var/log/containers/applog/*
  pos_file /var/log/es-containers-app.log.pos
  tag reform.*
  read_from_head true
</source>

<match reform.**>
  type record_reformer
  renew_record false
  enable_ruby true
  tag kubernetes.${tag_parts[1]}.${tag_parts[2]}.${tag_parts[3]}.${tag_parts[6]}.log
  <record>
    logvolume ${tag_parts[5]}
    filename ${tag_suffix[8]}
    log ${message}
    stream file
  </record>
  remove_keys message
</match>


<filter kubernetes.**>
  type kubernetes_metadata
</filter>

<match kubernetes.**>
  type record_reformer
  time_nano ${t = Time.now; ((t.to_i * 1000000000) + t.nsec).to_s}
  tag ${tag_suffix[1]}
</match>

<match **>
   type elasticsearch
   log_level info
   include_tag_key true
   host elasticsearch-logging
   port 9200
   logstash_format true
   # Set the chunk limit the same as for fluentd-gcp.
   buffer_chunk_limit 2m
   # Cap buffer memory usage to 512KB/chunk * 128 chunks = 65 MB
   buffer_queue_limit 128
   flush_interval 2s
   # Never wait longer than 5 minutes between retries.
   max_retry_wait 300
   # Disable the limit on the number of retries (retry forever).
   disable_retry_limit
</match>
```


<a id="orgd0dbd52"></a>

# 扩展阅读


<a id="org4011f94"></a>

## fluentd process nested JSON

-   Process nested JSON with Fluentd - Stack Overflow <https://stackoverflow.com/questions/35621964/process-nested-json-with-fluentd>


<a id="org7f12289"></a>

### eg

-   nested json
    
    ```js
    {"event" : {"name" : "toto", "date" : 14....11}}
    ```

-   correct fluentd config need enable<sub>ruby</sub>, and modify record
    
    ```yaml
    <filter **>
      @type record_transformer
      enable_ruby
      <record>
        event_date ${time.strftime('%Y-%m-%dT%H:%M:%S')}
    
      </record>
    
    </filter>
    ```


<a id="org4d36401"></a>

## 参考


<a id="org77adda8"></a>

### sonots/fluent-plugin-record-reformer

<https://github.com/sonots/fluent-plugin-record-reformer>


<a id="org8427bdb"></a>

### 日志收集工具 Fluentd 使用总结

<http://www.imekaku.com/2016/09/26/fluentd-conclusion/>


<a id="org7b81aee"></a>

### k8s 应用内部日志收集

<http://blog.csdn.net/ptmozhu/article/details/53132942>


<a id="org80b72cd"></a>

### AliyunContainerService/fluentd-pilot:

<https://github.com/AliyunContainerService/fluentd-pilot>
