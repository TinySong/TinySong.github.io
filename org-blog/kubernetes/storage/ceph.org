---
title: "Ceph luminous Install"
subtitle: "Ceph"
date: 2017-12-25T15:30:00+08:00
tags: ["ceph","luminous"]
type: "post"
categories: ["ceph","storage"]
description: ""
---

* ceph luminous install on centos

** 前期准备
*** 内核版本
    目前 ceph 官方提供的建议是，内核版本要求在 4.1.4 or later, 最低要求是 3.10.*,
    当然高版本可支持的 ceph 的功能更加完善，主要还是要看业务需要，再升级内核,
    kernel 版本对应的新功能的对照表：
     | 功能                              | 内核版本      |
     | firefly(CRUSH_TUNSBLES3) 可调选项 | 3.15 开始支持 |
     | B-tree 文件系统(Btrfs)             | 3.14 or later |
     |                                   |               |
**** 注意事项
     + 默认内核 btrfs 版本较老，不推荐用于 ceph-osd 存储节点，要升级到推荐的内核，或
       者改用 xfs, ext4
     + 默认内核带的 ceph 客户端较老，不推荐做内核空间客户端(内核 RBD 或 Ceph 文件系
       统),需要升级内核
     + 默认内核或已安装的 glibc 版本若不支持 rsncfs(2)系统调用，同一个机器上使用 xfs
       或 ext4 的 ceph-osd 守护进程性能不会如愿。
     + 从 Infernalis 版起，用户名 “ceph” 保留给了 Ceph 守护进程。如果 Ceph 节点上已经有了 “ceph” 用户，升级前必须先删掉这个用户。
**** 升级内核
*** 安装 luminous 源
    需要将以下 luminous 源写入 *各个节点* /etc/yum.conf.d/ceph.repo 中
    #+BEGIN_SRC sh
      [ceph]
      name=ceph
      baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/x86_64/
      gpgcheck=0
      [ceph-noarch]
      name=cephnoarch
      baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/noarch/
      gpgcheck=0
    #+END_SRC
    执行 ~yum makecache~ 命令，刷新安装包缓存
*** 安装 ceph 部署工具
    #+BEGIN_SRC sh
      yum install ceph-deploy
    #+END_SRC
    *注意：只在 master 节点上安装，slave 节点不需要安装*
*** ceph 节点安装
**** 安装 ntp
     *建议在所有 Ceph 节点上安装 NTP 服务（特别是 Ceph Monitor 节点），以免因时钟漂移导致故障*
     #+BEGIN_SRC sh
       yum install ntp ntpdate ntp-doc
     #+END_SRC
     设置 crontab 自动同步时间 */13 * * * * /usr/sbin/ntpdate -u
     time.cbdtelecom.cn
**** 创建部署 ceph 的用户
      见[[http://docs.ceph.org.cn/start/quick-start-preflight/][ceph 官方文档]]
      在 *各个* 节点上通过以下命令创建一个 cephtest 用户，
      #+BEGIN_SRC sh
        useradd -d /home/cephtest -m cephtest
        passwd cephtest
        echo "cephtest ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephtest
        chmod 0440 /etc/sudoers.d/cephtest
      #+END_SRC
** 安装过程
*** ceph 集群
    创建集群时，若未安装过 ceph 集群则不需要执行 purgedata 和 purge 命令, 若时重装的
    话，需要执行，清除之前的数据，防止造成数据污染
**** purgedata
     #+BEGIN_SRC sh
       # 如果只想清除 /var/lib/ceph 下的数据、并保留 Ceph 安装包
       ceph-deploy purgedata ceph-slave-15 ceph-slave-16
       # 要清理掉 /var/lib/ceph 下的所有数据、并卸载 Ceph 软件包
       ceph-deploy purge ceph-slave-15 ceph-slave-16
     #+END_SRC
     若节点上已经安装了 ceph,purgedata 时会报错，可
     以使用执行 ~ceph-deploy purge {ceph-node} [{ceph-node}]~,可以连 ceph 安装包一
     起清除。

**** ceph 集群初始化

     #+BEGIN_SRC sh
       ceph-deploy new ceph-master-11
       #上步会创建一个 ceph.conf 配置文件和一个监视器密钥环到各个节点的/etc/ceph/目录，ceph.conf 中会有`fsid`，`mon_initial_members`，`mon_host`三个参数#默认 ceph 使用集群名 ceph，可以使用下面命令创建一个指定的 ceph 集群名称
       #ceph-deploy --cluster {cluster-name} new {host [host], ...}
     #+END_SRC
      Ceph Monitors 之间默认使用 6789 端口通信，OSD 之间默认用 6800:7300 范围内
      的端口通信，多个集群应当保证端口不冲突
**** ceph 软件包安装
      通过以下命令，安装 ceph 软件包到三个节点上
       #+BEGIN_SRC sh
         ceph-deploy install --release luminous ceph-master-11 ceph-slave-15 ceph-slave-16
     #+END_SRC
     注意点需要添加 ~--release luminous~ ，为了安装指定的版本

**** 添加 mons
     #+BEGIN_SRC sh
       ceph-deploy mon create-initial
     #+END_SRC
     执行完后，默认在当前目录下生成以下文件列表
     #+BEGIN_SRC sh
       [root@ceph-master-11 luminous-deploy]# ls
       ceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log
       ceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring
     #+END_SRC
     执行 ceph -s 时，会提示错误

     #+BEGIN_SRC sh
       root@ceph-master-11 luminous-deploy]# ceph -s
      2017-12-25 18:38:19.452958 7fd10a8fe700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
      2017-12-25 18:38:19.452994 7fd10a8fe700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication
      2017-12-25 18:38:19.452997 7fd10a8fe700  0 librados: client.admin initialization error (2) No such file or directory
     #+END_SRC
     这里需要将 ceph.client.admin.keyring 同步到各个节点的/etc/ceph 目录下，并设
     置可读权限 : ~chmod +r /etc/ceph/ceph.client.admin.keyring~
     2. 将*.keyring 同步到其他 node 节点上

        #+BEGIN_SRC sh
          ceph-deploy admin ceph-master-11 ceph-slave-15 ceph-slave-16
        #+END_SRC

**** 添加 osd
      OSD node 是真正存储数据的节点，我们需要为 ceph-osd 提供独立存储空间，一般是一
      个独立的 disk。但我们环境不具备这个条件，于是在本地盘上创建了个目录，提供给
      OSD。
      分别登陆到两个 slave 节点执行以下命令
      #+BEGIN_SRC sh
        ssh ceph-slave-15
        sudo mkdir /ceph-data/osd0
        sudo chown -R ceph:ceph /ceph-data/osd0       # 由于 ceph 默认使用 ceph 用户名和用户组启动 ceph 服务，不进行设置的话，会造成 osd 无法启动
        exit
        ssh ceph-slave-16
        sudo mkdir /ceph-data/osd1
        sudo chown -R ceph:ceph /ceph-data/osd1
        exit
      #+END_SRC
      创建 osd
      #+BEGIN_SRC sh
        ceph-deploy osd create ceph-slave-16:/ceph-data/osd1 ceph-slave-15:/ceph-data/osd0 # (create equal prepare and active)
      #+END_SRC

**** 检查 ceph 状态
     #+BEGIN_SRC sh
       [root@ceph-slave-15 ~]# ceph -s
       cluster:
       id:     1eaa46c4-67c2-4a22-a1c3-f6fde2915a4e
       health: HEALTH_WARN
       no active mgr

       services:
       mon: 1 daemons, quorum ceph-master-11
       mgr: no daemons active
       osd: 2 osds: 2 up, 2 in               # osd 启动正常

       data:
       pools:   0 pools, 0 pgs
       objects: 0 objects, 0 bytes
       usage:   0 kB used, 0 kB / 0 kB avail
       pgs:

       [root@ceph-slave-15 ~]# ceph osd tree
       ID CLASS WEIGHT  TYPE NAME              STATUS REWEIGHT PRI-AFF
       -1       0.01959 root default
       -3       0.00980     host ceph-slave-15
       0   hdd 0.00980         osd.0              up  1.00000 1.00000
       -5       0.00980     host ceph-slave-16
       1   hdd 0.00980         osd.1              up  1.00000 1.00000

     #+END_SRC
    至此，ceph 集群基本组件已经安装完成
***  卸载 ceph 集群
    #+BEGIN_SRC sh
      #卸载指定节点上的 ceph 软件包
      ceph-deploy uninstall {hostname1 hostname2 ...}
      #清除数据
      #如果只想清除 /var/lib/ceph 下的数据、并保留 Ceph 安装包
      ceph-deploy purgedata [hostname1 hostname2 ...]

      #要清理掉 /var/lib/ceph 下的所有数据、并卸载 Ceph 软件包
      ceph-deploy purge [hostname1 hostname2 ...]
    #+END_SRC
    当重装时，可通过以上命令删除后，再次重新部署。
** ceph 块设备
   由于业务的原因，这里只用到了块设备，ceph 块设备利用 RADOS 实现了快照、复制和一
   致性。Ceph 的 RADOS 块设备（RBD）使用内核模块或 librbd 库与 OSD 交互
   *Note 内核模块可使用 Linux 页缓存。对基于 librbd 的应用程序，Ceph 可提供 [[http://docs.ceph.org.cn/rbd/rbd-config-ref/][RBD
   缓存]] *,ceph 集群可以同事运行 Ceph Rados 网关、ceph FS 文件系统、Ceph 块设备。

   osd 服务启动后，则可以通过以下命令创建 pool
   #+BEGIN_SRC sh
     ceph osd pool create tenx-pool 256 256
   #+END_SRC
   对于 pool 的一些命令操作可参考[[http://docs.ceph.org.cn/man/8/rbd/][rbd 官网]] 的博客
   ceph 修改 PG 数量: http://thinnote.com/archives/1718
** ceph-mgr
    http://www.zphj1987.com/2017/06/25/ceph-luminous-new-dashboard/
*** luminous 重要的变化
    + 默认的消息处理从 simple 变成了 async 了（ms_type = async+posix）
    + 默认的后端存储从 filestore 变成了 bluestore 了

** ceph-rest-api
*** service start
    ceph-rest-api -c /etc/ceph/ceph.conf -n client.admin --cluster ceph
*** restapi 操作
    1. 查看所有 restapi
       http://192.168.0.4:5000/api/v0.1
    2.查看 ceph 用户
    radosgw-admin user info --uid=johndoe

**** 例子
     1. 获取 api 用户系统
     curl http://0.0.0.0:5000/api/v0.1/auth/get?entity=client.admin
** 引用
   1. 使用 Ceph RBD 为 Kubernetes 集群提供存储卷 | Tony Bai
 	 http://tonybai.com/2016/11/07/integrate-kubernetes-with-ceph-rbd/
   2. 使用 ceph-deploy 工具部署 ceph 集群 | opengers
 	 http://www.isjian.com/ceph/deploy-a-ceph-cluster-use-ceph-deploy/#%E7%8E%AF%E5%A2%83%E9%A2%84%E6%A3%80
   3. 预检 — Ceph Documentation
 	 http://docs.ceph.org.cn/start/quick-start-preflight/

** 版本更新
   1. Ceph v12.2 Luminous 正式版本发布 – Ceph 国内社区
 	 http://ceph.org.cn/2017/08/30/ceph-v12-2-luminous%E6%AD%A3%E5%BC%8F%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/

** 升级
   1. 升级 Ceph 集群从 Kraken 到 Luminous — 青蛙小白
 	 https://blog.frognew.com/2017/11/upgrade-ceph-from-kraken-to-luminous.html#3%E6%9B%B4%E6%96%B0%E5%90%84%E8%8A%82%E7%82%B9ceph%E8%BD%AF%E4%BB%B6%E5%8C%85
